{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "\u001b[K     |████████████████████████████████| 232 kB 407 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.10.0-cp39-cp39-macosx_11_0_arm64.whl (3.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2 MB 322 kB/s eta 0:00:011\n",
      "\u001b[?25hCollecting tiktoken\n",
      "  Downloading tiktoken-0.9.0-cp39-cp39-macosx_11_0_arm64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 314 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from PyPDF2) (4.12.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (2.0.2)\n",
      "Requirement already satisfied: packaging in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from faiss-cpu) (24.2)\n",
      "Collecting regex>=2022.1.18\n",
      "  Downloading regex-2024.11.6-cp39-cp39-macosx_11_0_arm64.whl (284 kB)\n",
      "\u001b[K     |████████████████████████████████| 284 kB 409 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.26.0 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/pathaoltd/Library/Python/3.9/lib/python/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Installing collected packages: regex, tiktoken, PyPDF2, faiss-cpu\n",
      "Successfully installed PyPDF2-3.0.1 faiss-cpu-1.10.0 regex-2024.11.6 tiktoken-0.9.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.0.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install PyPDF2 faiss-cpu tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pathaoltd/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from constants import openai_key\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the path of  pdf file/files.\n",
    "pdfreader = PdfReader('Paper1.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Concatenate\n",
    "\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'BRIDGING CLASSICAL AND QUANTUM MACHINE LEARNING : KNOWLEDGE\\nTRANSFER FROM CLASSICAL TO QUANTUM NEURAL NETWORKS USING\\nKNOWLEDGE DISTILLATION\\nRESEARCH ARTICLE\\nMohammad Junayed Hasan1and M.R.C. Mahdy1, *\\n1Department of Electrical and Computer Engineering, North South University, Bashundhara, Dhaka\\nABSTRACT\\nVery recently, studies have shown that quantum neural networks surpass classical neural networks in\\ntasks like image classification when a similar number of learnable parameters are used. However,\\nthe development and optimization of quantum models are currently hindered by issues such as qubit\\ninstability and limited qubit availability, leading to error-prone systems with weak performance.\\nOn the other hand, classical models can exhibit high-performance owing to substantial resource\\navailability. As a result, more studies have been focusing on hybrid classical-quantum integration to\\nleverage the advantages of both paradigms. A line of research particularly focuses on transfer learning\\nthrough classical-quantum integration or quantum-quantum approaches. Unlike previous studies, this\\npaper introduces a new method to transfer knowledge from classical to quantum neural networks\\nusing knowledge distillation, effectively bridging the gap between classical machine learning and\\nemergent quantum computing techniques. We adapt classical convolutional neural network (CNN)\\narchitectures like LeNet and AlexNet to serve as teacher networks, facilitating the training of student\\nquantum models by sending supervisory signals during backpropagation through KL-divergence. The\\napproach yields significant performance improvements for the quantum models by solely depending\\non classical CNNs, with quantum models achieving an average accuracy improvement of 0.80%\\non the MNIST dataset and 5.40% on the more complex FashionMNIST dataset. Applying this\\ntechnique eliminates the cumbersome training of huge quantum models for transfer learning in\\nresource-constrained settings and enables re-using existing pre-trained classical models to improve\\nperformance. Thus, this study paves the way for future research in quantum machine learning (QML)\\nby positioning knowledge distillation as a core technique for advancing QML applications.\\nKeywords Parameterized quantum circuits ·Quantum superposition ·Knowledge distillation ·Convolutional neural\\nnetworks ·Classification\\n1 Introduction\\nQuantum machine learning (QML) holds great promise to revolutionize computational paradigms by offering unprece-\\ndented computational speedups for certain tasks through parallel computation leveraging quantum superposition and\\nentanglement. In particular, quantum neural networks (QNNs) have shown promising results compared to classical\\napproaches by outperforming classical models in image classification tasks and converging faster when similar number\\nof learnable parameters are used [ 1,2]. Moreover, well-constructed QNNs can have a substantially higher effective\\ndimension than classical neural networks, implying a greater capacity for quantum networks to handle complex data\\nstructures and tasks [3]. Despite the promising advancements recently, the infancy of QML poses intrinsic challenges.\\nAlthough they are theoretically potent, the development and optimization of QNNs are often constrained by factors\\nsuch as quantum bit (qubit) stability, coherence times, and restricted qubit availability [ 4,5]. This results in a system\\nthat is error-prone, limiting the complexity and expressive power of models that can be implemented and evaluated on\\nactual quantum hardware [ 6,7,8]. This embryonic stage of QML unavoidably collides with the immense computational\\n* Corresponding author. E-mail address : mahdy.chowdhury@northsouth.edu (M.R.C. Mahdy).arXiv:2311.13810v1  [quant-ph]  23 Nov 2023RESEARCH ARTICLE\\nburden required to train highly-accurate models, a situation further worsened by the inherently stochastic nature of\\nquantum computations and the concurrent need for substantial resources to manage and mitigate resultant quantum\\nnoise.\\nExisting approaches to handle the errors and resource-limitations of QNNs include neural error mitigation, variational\\nquantum-neural hybrid error mitigation, resource-efficient quantum circuit design, quantum-to-quantum transfer learning\\nand classical-to-quantum transfer learning. However, each of these approaches has its own limitations. Neural error\\nmitigation requires a large amount of training data and computational resources to train the neural networks that correct\\nthe quantum errors[ 9]. Variational quantum-neural hybrid error mitigation introduces additional variational parameters\\nthat need to be optimized, which can increase the complexity and instability of the quantum optimization process.\\nResource-efficient quantum circuit design depends on the specific choice of the linear transformation and the quantum\\nfeature map, and may not be applicable to arbitrary quantum tasks [ 10]. Quantum-to-quantum transfer learning assumes\\nthat the source and target quantum systems or tasks are sufficiently similar, and may not work well for heterogeneous\\nor diverse quantum domains. Classical-to-quantum transfer learning relies on the classical neural network to extract\\nmeaningful features from the data, which may not capture the quantum correlations or entanglement that are essential\\nfor quantum advantages.\\nTransfer learning, especially using knowledge distillation [ 11], effectively navigates these challenges by offering\\na straightforward alternative where large pre-trained models share their learned knowledge with smaller ones in a\\nteacher-student setup. This approach obviates the necessity of burdensome training and error mitigation to improve\\nmodel performance by depending solely on the logits of large pre-trained models, assuaging the computational cost.\\nFurthermore, this method also circumvents the limitations of the available resources by enabling smaller models to\\ninherit robust, pre-optimized decision boundaries and representational features from the teacher models. KD can be\\nused to transfer the knowledge learned by a large or complex neural network (the teacher) to a smaller or simpler\\nneural network (the student). It can also reduce the noise sensitivity of the student network, and enable cross-domain or\\ncross-hardware applications. In addition, it can enhance the interpretability and explainability of the student neural\\nnetwork, by revealing the underlying physical rules or symmetries that govern the performance enhancement. As\\na result, knowledge distillation can be used and applied as a promising technique for neural network design and\\noptimization.\\nKnowledge distillation in neural networks can be accomplished in various ways: (1)Response-based knowledge\\ndistillation [ 12]: This approach transfers the knowledge from the teacher network to the student network by matching\\ntheir outputs or responses on a given dataset. The student network learns to mimic the teacher network’s predictions,\\nprobabilities, or logits, and thus inherits the teacher network’s generalization ability. (2)Feature-based knowledge\\ndistillation [ 12]: This approach transfers the knowledge from the teacher network to the student network by matching\\ntheir intermediate features or representations. The student network learns to extract similar features as the teacher\\nnetwork, and thus captures the teacher network’s discriminative power. (3)Relation-based knowledge distillation [ 12]:\\nThis approach transfers the knowledge from the teacher network to the student network by matching their relations or\\ninteractions among the inputs, outputs, or features. The student network learns to preserve the same relations as the\\nteacher network, and thus acquires the teacher network’s structural or semantic information.\\nRecent studies have explored transfer learning in quantum neural networks with respoonse-based knowledge distillation\\nusing a Quantum-to-quantum approach [ 9]. A large domain of Knowledge Distillation remains unexplored: Classical-to-\\nquantum knowledge transfer. Given the constraints of quantum machine learning, it is crucial to explore the possibilities\\nthat a quantum model can enhance its performance solely based on the outputs of classical neural networks. This leads\\nus to the research questions of this study, based on which we design the methodology, experiment rigorously, analyze\\nand conclude our findings in this paper:\\n•Research Question 1: Can we transfer the knowledge from a classical neural network (the teacher) to a\\nquantum neural network (the student) using knowledge distillation?\\n•Research Question 2: How does knowledge distillation from classical neural network to quantum neural\\nnetwork affect the performance of the quantum model?\\n•Research Question 3: Is it possible to improve the performance of the quantum models upon knowledge\\ndistillation from classical neural networks?\\nIn this paper, we show theoretically and experimentally that knowledge transfer from classical to quantum models using\\nknowledge distillation indeed improves the performance of the quantum models drastically. This method obviates\\nthe dependency of QNNs upon larger QNNs, thus eliminating the necessity of hectic model training, quantum error-\\nmitigation and resource handling for improving the performance of quantum models. To demonstrate our claim, we\\nconduct extensive experiments on two widely used datasets: MNIST [ 13] and FashionMNIST [ 14]. We use multiple\\nclassical convolutional neural networks as teacher models to train on these datasets and generate the outputs for use\\n2RESEARCH ARTICLE\\nby the quantum student models. We also use multiple quantum student models like 3-qubit and 4-qubit variational\\nquantum circuits, to learn from the outputs of the classical teacher models and improve performance. We compare\\nthe performance of the quantum student models with and without knowledge distillation, and show that knowledge\\ndistillation can significantly improve the accuracy, robustness, and efficiency of the quantum models. Moreover, we\\nalso use ensembles of the classical teacher models, and investigate how the performance of the quantum student models\\nvaries with the number of teacher models used. We find that using more teacher models can provide more diverse and\\ncomplementary information for the quantum student models, and thus further enhance the knowledge transfer effect.\\nThe methodology followed in this study is visualized in Figure 1. The figure shows that after following data pre-\\nprocessing steps for classical and quantum models respectively, the classical data is passed through a classical network\\ncontaining teacher CNN model(s), whereas the quantum data is passed through student QNN model(s). After that,\\nusing the outputs of the models, the KL-Divergence is calculated and added to the loss of the student model to calculate\\na total loss which is used to improve performance of the models. Adding the KL-divergence acts as a supervisory signal\\nwhile updating the parameters of the quantum model. In this teacher-student setup, the student QNN thus tries to mimic\\nthe outputs of the teacher CNN and knowledge transfer is accomplished from classical to quantum models solely based\\non the mimicry.\\nC N NC l a s s  P r o b a b i l i t i e s\\nP r o b a b i l i t y  D i s t r i b u t i o nH i g h e s t  \\nP r o b a b i l i t y\\nC l a s sN  \\nS h o t sQ N NK L  \\nD i v e r g e n c eN o r m a l i z a t i o nF i l t e r i n gR e s i z i n gA m p l i t u d e  E n c o d i n gD a t a s e tC l a s s i c a l  N e t w o r k[ 0  1  0  . . . .  1  0  1 ]\\n[ 0  1  0  . . . .  1  0  1 ]\\n\\n[ 0  1  0  . . . .  1  0  1 ]\\n[ 0  1  0  . . . .  1  0  1 ]. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .. . . . .\\nL o s s  \\nC a l c u l a t i o n\\n.  .  .  ..  .  .  .\\nQ u a n t u m  N e t w o r kD a t a  P r e - p r o c e s s i n gT o t a lL o s s\\nU p d a t e  P a r a m e t e r s\\nFigure 1: A visual representation of the research methodology followed in this research. At the very beginning, data\\npre-processing is carried out individually for classical and quantum model training. Following that, the data is passed\\nthrough the respective models. A pre-trained CNN is used in the classical network. The outputs from the CNN and the\\nQNN are used to calculate the KL-divergence and add that to the loss of the quantum network to aid in backpropagation.\\nContributions. The contributions of this research can be summarized as follows:\\n•We propose a novel hybrid knowledge distillation framework to transfer the knowledge from multiple classical\\nteacher models to quantum student models, which establishes the re-usability of existing pre-trained classical\\nmodels in quantum training and eliminates the necessity of cumbersome QNN training.\\n•We systematically investigate the effect of knowledge distillation from classical neural networks to quantum\\nneural networks on the performance of the quantum models.\\n•We conduct extensive experiments on two widely used datasets, MNIST and FashionMNIST, and compare the\\nperformance of various quantum student models with and without knowledge distillation.\\n•We empirically present that knowledge distillation can significantly improve the accuracy, robustness, and\\nefficiency of the quantum models, and also enable cross-domain and cross-hardware applications.\\n•We provide insights into the underlying mechanisms and factors that influence the knowledge transfer process,\\nand suggests future directions for research in this area.\\nThe rest of the paper is organized as follows: Section 2 discusses the related works, providing context and background\\nfor our approach. Section 3 describes the methodology, detailing the mathematical foundations and our approach to\\n3RESEARCH ARTICLE\\nknowledge distillation. In Section 4, we present the experiments and results, along with the datasets used, models\\nutilized and the specifics of our implementation. Section 5 presents the findings of this study and dives into their\\nin-depth discussion, interpreting the results and their implications. Section 6 concludes the paper and suggests directions\\nfor future work, highlighting the potential applications and extensions of our research. Finally, we acknowledge the\\ncontributions of the people that have assisted in this research.\\n2 Related Works\\n2.1 Quantum Machine Learning: Problems and Challenges\\nQuantum Machine Learning (QML) aspires to harness the properties of quantum mechanics to revolutionize computation,\\nbut the path is restricted with various challenges and obstacles. The concept of quantum supremacy, where quantum\\nsystems perform tasks beyond the reach of classical computers, has seen experimental strides as evidenced by [ 15]\\nand [ 16]. However, these pioneering steps also underscore the fragility of quantum states and their vulnerability to\\ndecoherence, a problem that Shor’s early work [ 17] sought to address through quantum error correction, a technique\\nstill pivotal in contemporary research [ 18]. The transient stability of qubits, a topic explored by [ 19], and the intricate\\nchallenge of manipulating quantum states, as demonstrated by [20], add layers of complexity to quantum computing.\\nThe Noisy Intermediate-Scale Quantum (NISQ) era, defined by the presence of noisy quantum bits, has constrained the\\nscalability of quantum computers, a concern that [ 21] aimed to tackle through the quantum Boltzmann machine, an\\napproach designed to work within these noisy environments. Concurrently, the development of quantum algorithms and\\nneural networks that can operate under such noise is the focus of [ 22] and [ 23], who seek methods to optimize QML\\nmodels despite these limitations. Data encoding poses a unique challenge in QML, as most algorithms are tailored for\\ninherently quantum data. [ 24] delve into creating quantum-enhanced feature spaces, while [ 25] look into reinforcement\\nlearning agents that can operate within the quantum realm. These studies offer insights into the potential application\\ndomains for QML, suggesting that despite current limitations, certain tasks may still be within reach. Furthermore, the\\npractical implementation of quantum computing and the exploration of its utility across various domains have been the\\nsubject of investigation by [ 26], who discuss the variational quantum algorithms that could be the cornerstone of future\\nquantum applications.\\nDespite the under-developed stage of QML, there is an ongoing effort to address its challenges. [ 27] provides a\\ncomprehensive analysis of the prospects and barriers that lie ahead, ensuring that the community remains grounded\\nin reality while exploring the vast potential of quantum technologies. The integration of efforts from all these studies\\nmanifests a commitment to overcoming the current hurdles, indicating a future where quantum and classical computing\\nmay converge to solve problems that were once thought unsolvable. In this research, we commit to this challenge\\nand address it using a hybrid classical-quantum architecture and demonstrate that the methodology takes handling the\\nchallenges faced in the realm of QML one step further.\\n2.2 Error Mitigation and Resource Handling in Quantum Computing\\nThe pursuit of reliable quantum computing is marked by a need to address the fundamental limits of quantum error\\nmitigation. Works such as those by [ 28] provide a foundational understanding of the theoretical bounds in quantum\\nsystems, crucial for error correction techniques in neural networks. This is further explored through the lens of scalability\\nand statistics of errors in recent advancements within noisy intermediate-scale quantum (NISQ) technologies [ 29].\\nReference-state error mitigation strategies [ 30] have shown promise in improving the accuracy of quantum computations,\\nwhich is vital for the fidelity of knowledge distillation processes. Moreover, the extension of computational reach\\nthrough error mitigation in noisy quantum processors [ 31] has direct implications for the robust execution of quantum\\nneural networks. The integration of learning-based techniques for quantum error mitigation [ 32] parallels the adaptive\\nnature of knowledge distillation, suggesting a methodological symmetry that could be harnessed for transferring\\nknowledge. The evaluation and benchmarking of error mitigation techniques are critical for determining the most\\neffective methods for application in quantum neural networks [ 33]. This selection is further informed by the development\\nof a unified approach to data-driven error mitigation [ 34], offering a pathway to harnessing empirical data in the error\\ncorrection process. Techniques for mitigating errors in quantum approximate optimization [ 35] can also inform the\\noptimization strategies within the knowledge distillation framework.\\nOn the front of resource handling, the importance of optimizing resource efficiencies is underscored by [ 36], drawing\\nattention to the balance between computational capabilities and resource constraints. This optimization is directly\\napplicable to the development of quantum neural networks, where the allocation and management of quantum resources\\nare crucial. Furthermore, the critical challenges of scaling quantum computation, as outlined by [ 37], must be addressed\\nto enable the practical implementation of fault-tolerant quantum neural networks. Insights into the cross-discipline\\n4RESEARCH ARTICLE\\ntheoretical research on quantum computing, such as those presented by [ 38], highlight the urgency and significance\\nof resource management in the context of knowledge distillation. Additionally, the potential applications of quantum\\ncomputing in specific domains like renewable energy optimization [ 39] illustrate the diverse utility and the resource\\nhandling strategies that may be required for energy-efficient quantum neural network operations. Differently from\\nprevious approaches, we completely nullify the need for QNN optimization using error mitigation or resource handling.\\nRather, keeping the student quantum model small and static, we depend fully on pre-trained classical models and their\\nensembles to improve the performance of the QNN, thus addressing the challenges of QML in a new, different way.\\n2.3 Transfer Learning in Quantum Machine Learning\\nTransfer learning has emerged as a potent strategy in classical machine learning, enabling models trained on one task to\\nbe repurposed for another related task [ 40,41]. In quantum machine learning (QML), this approach promises to address\\nscalability and adaptability challenges that are crucial in the practical application of neural-network quantum states\\n(NNQS) [ 40]. Scalability, a primary concern in QML, is particularly relevant for tasks that require models to extrapolate\\nlearned behaviors to larger quantum systems. Protocols inspired by physics have shown that features learned from\\nsmaller systems can significantly accelerate and refine the learning process when applied to more complex systems, as\\nevidenced in the one-dimensional transverse field Ising and Heisenberg XXZ models [ 40]. The versatility of transfer\\nlearning is further showcased by its application to the optimization of neural network potentials, exemplified by ANI-1x,\\nwhich was retrained to near gold-standard quantum mechanical accuracy. This highlights transfer learning’s broad\\npotential across disciplines, including materials science, biology, and chemistry [41, 42].\\nIntegrating classical and quantum neural networks has extended transfer learning protocols into classical-quantum\\nhybrid domains. By combining pre-trained classical networks with variational quantum circuits, researchers have\\nopened up new possibilities for processing high-dimensional data such as images before entangling them within the\\nquantum processing framework [ 43]. This hybrid approach not only utilizes the strengths of classical networks in\\ndata handling but also harnesses the quantum advantages for feature processing, potentially revolutionizing fields like\\nimage recognition and quantum state classification [ 43,44]. Moreover, the concept of knowledge distillation has been\\nintroduced into the realm of QML, allowing the creation of new quantum neural networks (QNNs) through approximate\\nsynthesis [ 9]. This methodology enables the reduction of circuit layers or the alteration of gate sets without the necessity\\nof training from scratch. Empirical analyses suggest that significant reductions in circuit complexity can be achieved\\nwhile simultaneously improving accuracy, even under noisy conditions, underscoring the effectiveness of transfer\\nlearning techniques in the development of QNNs [9].\\nIn terms of practical applications, transfer learning has been successfully applied to improve the accuracy and efficiency\\nof surrogate models used in design optimization [ 45], and disease detection [ 46,47], demonstrating its efficacy in\\nenhancing computational time and model performance. Furthermore, leveraging pre-existing computational datasets\\nthrough deep transfer learning constructs robust prediction models that outperform those based solely on theoretical\\ncalculations [ 48]. The domain of spoken command recognition (SCR) has also benefitted from the implementation\\nof hybrid transfer learning algorithms. By transferring knowledge from a pre-trained classical network to a hybrid\\nquantum-classical model, researchers have been able to significantly improve performance on SCR tasks, showcasing\\nthe practical benefits of transfer learning in QML [49].\\nQuantum Convolutional Neural Networks (QCNNs) are gaining popularity in both quantum and classical data classifi-\\ncation recently [ 50]. The adoption of transfer learning in QML is proposed to circumvent the scalability constraints\\nof quantum circuits in the noisy intermediate-scale quantum (NISQ) era. For instance, research has shown that small\\nQCNNs can effectively solve complex classification tasks by leveraging pre-trained classical CNNs, thus bypassing the\\nneed for expansive quantum circuitry [ 49]. This approach is validated through numerical simulations of QCNNs, which\\ndemonstrate a superior classification accuracy over classical models when pre-trained on different datasets. The synergy\\nbetween classical and quantum networks is evident when a classical CNN, trained on FashionMNIST data, is utilized to\\nenhance the performance of a QCNN for MNIST data classification [ 50]. The empirical results advocate for transfer\\nlearning from classical to quantum CNNs, revealing a performance that significantly outshines classical transfer learning\\nmodels under analogous training conditions. These findings not only elucidate the potential of knowledge transfer to\\noptimize the utilization of QCNNs in the NISQ era but also underscore the practicality of hybrid classical-quantum\\nmodels in overcoming the limitations posed by the present-day quantum technologies. Driven by these findings, our\\napproach is designed to transfer knowledge from classical CNNs to quantum models. However, differently from [ 49]\\nand [ 50], we use knowledge distillation and show empirically that knowledge transfer is possible across homogeneous\\ndatasets in quantum models by simply trying to mimic the soft targets of classical CNNs instead of hectic training by\\nfreezing the pre-trained model and transferring the knowledge to a QNN during training in the previous approaches.\\n5RESEARCH ARTICLE\\n3 Methodology\\n3.1 Problem Formulation and Integration of Knowledge Distillation\\nWe consider a classical neural network as the teacher model and a quantum neural network (QNN) as the student model.\\nThe teacher model is denoted by ft:X → Y , where Xis the input space and Yis the output space. Similarly, the\\nstudent model is denoted by fs:X → Y .\\nGiven a dataset D={(xi, yi)}N\\ni=1where xi∈ X andyi∈ Y, the task is to train the student QNN to approximate the\\nmapping of the teacher model. The performance of the QNN can be evaluated by a loss function L:Y × Y → R,\\nwhich measures the discrepancy between the predictions of the QNN and the ground truth labels. The objective function\\nfor the QNN is given in Equation 1 as:\\nO(fs) =1\\nNNX\\ni=1L(fs(xi), yi). (1)\\nThe loss of the model outputs, L, is calculated using cross-entropy loss [51] according to Equation 2:\\nLcross-entropy (y,ˆy) =−CX\\ni=1yilog(ˆyi) (2)\\nwhere Cis the total number of classes, yiis the ground truth label indicating the probability of the predicted class, and\\nˆyiis the predicted probability that the observation is of class i.\\nKnowledge Distillation (KD) aims to transfer knowledge from the teacher model to the student model by minimizing\\na distillation loss LKD. The KD process involves a soft target τwhich controls the smoothness of the teacher’s\\noutput probability distribution. The distillation loss can be defined using the Kullback-Leibler (KL) divergence [ 52] in\\nEquation 3 as:\\nLKD(fs, ft) =1\\nNNX\\ni=1KL\\x10\\nσ\\x00ft(xi)\\nτ\\x01\\n||σ\\x00fs(xi)\\nτ\\x01\\x11\\n, (3)\\nwhere σdenotes the softmax function and KL denotes the Kullback-Leibler divergence. The softmax function is given\\naccording to Equation 4 by:\\nσ(z)i=ezi\\nPK\\nj=1ezj(4)\\nwhere σ(z)iis the output of the softmax function for the i-th class, ziis the i-th element of the input vector z, andKis\\nthe total number of classes. The exponential function eziensures that all outputs are non-negative and the denominator\\nnormalizes the values to ensure they sum up to 1, thus forming a valid probability distribution.\\nAnd, the KL-divergence is calculated as per Equation 5:\\nKL(P||Q) =KX\\ni=1P(i) log\\x12P(i)\\nQ(i)\\x13\\n(5)\\nwhere PandQare discrete probability distributions. P(i)represents the probability of the i-th class according to the\\ntrue label distribution, typically obtained from the softmax function of the teacher model σ(ft(xi)), and Q(i)is the\\nprobability of the i-th class according to the predicted label distribution from the student model σ(fs(xi)).Kis the\\ntotal number of classes. The KL divergence measures the difference between two probability distributions over the\\nsame variable. The total loss for the student QNN, incorporating both the original objective and the distillation loss, is\\ngiven in Equation 6 by:\\nOtotal(fs) =O(fs) +λLKD(fs, ft), (6)\\nwhere λis a hyperparameter that balances the importance of the original loss and the distillation loss.\\nThe student QNN is trained by optimizing Ototal through a quantum optimization algorithm. The goal of the\\noptimization is to minimize the loss, which can be formulated in Equation 7 as:\\nf∗\\ns= arg min\\nfsOtotal(fs). (7)\\n6RESEARCH ARTICLE\\n3.2 Quantum Superposition and the Mathematical Intuition Behind QNNs\\nA QNN can be represented by a parameterized quantum circuit (PQC) U(θ), where θdenotes the vector of parameters.\\nThe output of the QNN for an input state |ψ(x)⟩prepared by a quantum feature map is given in Equation 8 by:\\nfs(x) =⟨ψ(x)|U†(θ)OU(θ)|ψ(x)⟩, (8)\\nwhere Ois an observable corresponding to the output measurement.\\nThe postulates of quantum mechanics [ 53] allow us to describe the state of a quantum system using a wave function or\\nstate vector in a Hilbert space. A quantum system in a state |ψ⟩can exist in a superposition of different basis states\\n{|i⟩}. This superposition is mathematically represented in Equation 9 as:\\n|ψ⟩=X\\nici|i⟩, (9)\\nwhere ciare complex coefficients satisfying the normalization conditionP\\ni|ci|2= 1.\\nThe probability P(i)of the system being found in a particular basis state |i⟩upon measurement is given by the square\\nof the modulus of the coefficient corresponding to that state: P(i) =|ci|2\\nThe expectation value ⟨O⟩of an observable O, which corresponds to a physical quantity that can be measured, is\\ncomputed according to Equation 10 as:\\n⟨O⟩=⟨ψ|O|ψ⟩=X\\ni,jc∗\\nicj⟨i|O|j⟩. (10)\\nFor an observable Owith eigenstates |ok⟩and eigenvalues ok, the expectation value takes the form of Equation 11:\\n⟨O⟩=X\\nkokP(ok), (11)\\nwhere P(ok)is the probability of obtaining the eigenvalue okupon measuring the observable O.\\nIn the context of quantum computing, particularly in quantum neural networks, the output of the network can be\\nassociated with the expectation value of a certain observable. By designing the observable suitably, the expectation\\nvalues can be interpreted as probabilities, providing us with a powerful tool to map quantum computations to classical\\noutputs useful for tasks such as classification. This probabilistic interpretation is the key to the integration of quantum\\nsystems with machine learning, as it allows us to use quantum computations to produce outputs that can be understood\\nand utilized within the classical framework of neural networks.\\nThe knowledge transfer from the classical teacher model to the quantum student model is achieved by the optimization\\nof the quantum circuit parameters θto minimize Ototal. This optimization can be performed using gradient-based\\nmethods where the gradient can be estimated via the parameter shift rule or other quantum backpropagation techniques.\\n3.3 Quantum Gradient Descent and Parameter Optimization\\nIn the training of quantum neural networks (QNNs), the optimization of parameters θis crucial. Due to the nature of\\nquantum computing, traditional backpropagation as used in classical neural networks is not directly applicable. Instead,\\nwe utilize the parameter shift rule to compute gradients of quantum circuits.\\n3.3.1 Parameter Shift Rule\\nThe parameter shift rule is a method to compute the gradient of an expectation value of an observable with respect to\\nthe parameters of a quantum circuit [ 54]. For a parameter θjand an observable O, the gradient can be calculated using\\nthe following Equation 12 as:\\n∂⟨O⟩\\n∂θj=1\\n2\\x02\\n⟨O⟩θj+π\\n2− ⟨O⟩θj−π\\n2\\x03\\n, (12)\\nwhere ⟨O⟩θj±π\\n2denotes the expectation value of Owhen the parameter θjis shifted by ±π\\n2respectively.\\n7RESEARCH ARTICLE\\n3.3.2 Quantum Gradient Descent\\nOnce the gradients are computed, the parameters can be updated using a gradient descent algorithm. The update rule at\\neach iteration tis given by Equation 13:\\nθ(t+1)\\nj =θ(t)\\nj−η∂Ototal\\n∂θj, (13)\\nwhere ηis the learning rate.\\nThe learning rate can be fixed or adaptively changed according to specific strategies, such as learning rate annealing\\n[55] or using advanced optimizers like Adam [ 56], RMSprop [ 57], or AdaGrad [ 58], which are adapted for the quantum\\ndomain.\\n3.3.3 Quantum Circuit Learning\\nThe training process involves iteratively adjusting the parameters θto minimize the loss function Ototal. This is\\nreferred to as quantum circuit learning and is given by Equation 7. The optimization process leverages the quantum\\nsuperposition and entanglement to explore the parameter space more efficiently than classical methods. The inherent\\nprobabilistic nature of quantum measurements is accounted for in the optimization loop, making the training resilient to\\nthe stochastic nature of quantum mechanics. By integrating these quantum-specific optimization techniques, we aim to\\nexploit the full potential of quantum neural networks while addressing the challenges posed by the hardware limitations\\nand the unique aspects of quantum computation.\\n3.4 Amplitude Encoding in Quantum Neural Networks\\nAmplitude encoding [ 59] is a technique used in quantum computing to map classical data into quantum states. This\\nmethod leverages the ability of quantum states to exist in superpositions, allowing an exponential compression of data\\ncompared to classical representations.\\nA classical vector x∈Rncan be normalized and encoded into the amplitudes of a quantum state |ψ⟩in a Hilbert space\\noflog2(n)qubits, given according to Equation 14:\\n|ψ⟩=n−1X\\ni=0xi|i⟩, (14)\\nwhere {xi}are the normalized values of the classical vector x, and{|i⟩}represents the computational basis states.\\nThe normalization ensures thatPn−1\\ni=0|xi|2= 1, which is a requirement for any valid quantum state.\\nAmplitude encoding is necessary for efficiently utilizing the limited number of qubits available in current quantum\\nhardware. It allows the encoding of a classical dataset with 2nfeatures into a quantum system with only nqubits, thus\\nexponentially reducing the dimensionality of the data representation.\\nIn practice, amplitude encoding can be implemented using various quantum gates and operations. For instance, the\\nnon-linear transformation given by x′=π·tanh( x)can be applied to preprocess the data before encoding it into the\\namplitudes. Here, x′denotes the transformed feature that is to be encoded into the quantum state. The hyperbolic\\ntangent function ensures that the features are scaled to the range [−π, π], which is suitable for encoding into quantum\\nstate amplitudes. Then, we can map these preprocessed classical features into the amplitudes of a quantum state\\naccording to Equation 15:\\n|ψ(x)⟩=1\\n∥x′∥n−1X\\ni=0x′\\ni|i⟩, (15)\\nwhere ∥x′∥is the L2 norm of the vector x′.\\nThe encoding process translates into the preparation of quantum states through a sequence of quantum gates that\\ntransform the initial state |0⟩⊗ninto the desired superposition state |ψ(x)⟩. The specific sequence of gates depends\\non the quantum hardware and the form of the feature vector. In quantum neural networks, amplitude encoding serves\\nas the crucial first step that translates classical information into a form that quantum layers can process, leading to\\nthe exploitation of quantum properties such as superposition and entanglement in subsequent computations. By using\\n8RESEARCH ARTICLE\\namplitude encoding, we ensure that classical data is compatible with quantum processing, allowing the QNN to leverage\\nthe advantages of quantum computation in solving machine learning tasks.\\n4 Experiments and Results\\n4.1 Datasets Used\\nIn our experiments, we have employed two well-known datasets: MNIST and FashionMNIST. The datasets are shown\\nin Figure 2. Two datasets are used to demonstrate the generalizability and broader application of the methodology in\\nthis study.\\n4.1.1 MNIST\\nThe MNIST dataset [ 13] is a large database of handwritten digits that is commonly used for training various image\\nprocessing systems. It contains 60,000 training images and 10,000 testing images, each of which is a grayscale image\\nof size 28 ×28 pixels. The dataset is designed to allow researchers to benchmark their algorithms in classifying and\\nrecognizing handwritten numerical digits from 0 to 9. Due to its simplicity and the fact that it has been extensively\\nstudied, it serves as a standard for evaluating the performance of various machine learning techniques.\\n4.1.2 FashionMNIST\\nThe FashionMNIST dataset [ 14] is a more recent dataset comprising 60,000 training samples and 10,000 test samples.\\nEach sample is a 28 ×28 grayscale image, associated with a label from 10 classes. The classes represent different types\\nof clothing and fashion items, such as T-shirts/tops, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags,\\nand ankle boots. This dataset is intended to serve as a direct drop-in replacement for the original MNIST dataset for\\nbenchmarking machine learning algorithms, with the added complexity of distinguishing between different types of\\nclothing.\\nBoth datasets are preprocessed in a similar manner, normalized, and used as the basis for training and testing our models.\\nBy utilizing these datasets, we aim to show that our approach is not limited to a specific type of data or domain but can\\nbe applied to various image recognition tasks, highlighting its flexibility and wide applicability.\\n(a)\\n (b)\\nFigure 2: The datasets used in the experiments. (a) The MNIST dataset of written digits having 10 classes. (b) The\\nFashionMNIST dataset of clothing containing 10 classes.\\n4.2 Dataset Filtration\\nIn line with previous research that has addressed similar challenges in quantum machine learning [ 60,9,50], we filtered\\nthe MNIST and FashionMNIST datasets to reduce simulation time and focus on specific aspects of classification tasks.\\nWe created two subsets: MNIST 0-5, containing only the classes labeled from 0 to 5, and FashionMNIST 0-7, including\\nclasses from 0 to 7. Furthermore, within each class, we have randomly selected 150 samples to form the filtered datasets.\\nThe filtration of datasets serves several purposes in our study. Firstly, it allows us to evaluate our method’s effectiveness\\nacross a range of classes while keeping computational resources manageable. Quantum training can be inherently\\n9RESEARCH ARTICLE\\nslow and resource-intensive due to the current limitations of quantum hardware and the complex nature of quantum\\ncomputations. By reducing the number of classes and samples, we aim to demonstrate the feasibility of training\\nquantum models without the need for extensive resources, which is crucial for the practical adoption of quantum\\nmachine learning.\\nChoosing a smaller subset of samples is motivated by the need for computational efficiency. Quantum computers and\\nsimulators, particularly those accessible for research purposes, are subject to resource constraints such as limited qubits\\nand short coherence times. These constraints make training on full datasets impractical. The selection of 150 samples\\nper class allows for a diverse enough representation to train on and validate our models, ensuring that the performance\\nmetrics reflect the models’ ability to generalize from a limited sample size.\\n4.3 Classical Convolutional Neural Networks\\nConvolutional Neural Networks (CNNs) have been paramount in the field of image recognition and classification. For\\nthe MNIST and FashionMNIST datasets, two architectures stand out due to their historical significance and performance:\\nLeNet [ 61] and AlexNet [ 62]. In our experiments, we have used these two models individually and their ensemble by\\naveraging the outputs of the final layers as classical teacher models. Both LeNet and AlexNet were originally intended\\nfor higher dimensional images. To accommodate the 28 ×28 pixel grayscale images of MNIST and FashionMNIST, we\\nhave modified the input layers of these models to accept single-channel 28 ×28 pixel input instead of their original input\\ndimensions.\\n4.3.1 LeNet\\nLeNet is one of the earliest convolutional neural networks that significantly impacted the field of deep learning. It is a\\nmuch simpler network compared to AlexNet, consisting of two convolutional layers followed by two fully connected\\nlayers. LeNet is particularly well-suited for handwritten digit recognition tasks like MNIST due to its architecture being\\ndesigned for this purpose. In our experiments, we have used the original LeNet architecture by changing the input layer\\nto accept images of dimension 28 ×28 instead of its original input image dimension of 32 ×32, as shown in Figure 3.\\nThe intermediate layers were constructed according to the original architecture with no change.\\nFigure 3: The modified LeNet architecture used in the implementation of the experiments. Two convolution layers,\\neach followed by tanh activation and average pool layers were used. Finally, three fully connected layers were used to\\nclassify the images and get output probabilities from the model.\\n4.3.2 AlexNet\\nAlexNet is a deep CNN that marked a breakthrough in the ImageNet competition. It consists of five convolutional layers\\nfollowed by three fully connected layers. AlexNet takes an RGB image of size 256 ×256 as input and uses 11 ×11 filters\\nwith a stride of 4 in the first convolutional layer. Since MNIST and FashionMNIST images are grayscale images of size\\n28×28, the architecture of AlexNet was modified in the intermediate layers. Figure 4 shows the followed architecture\\nof AlexNet that was used instead: The modified architecture features an initial convolutional layer with 64 filters of size\\n3×3, stride 1, and padding 1, followed by ReLU activation and a max pooling layer with a 2 ×2 kernel and stride 2. The\\nsecond layer has 192 filters of size 3 ×3 with padding 1, followed by ReLU activation and another 2 ×2 max pooling.\\nThe third convolutional layer consists of 384 filters with a kernel size of 3 ×3 and padding of 1, followed by ReLU\\n10RESEARCH ARTICLE\\nactivation. This is followed by a series of fully connected layers: the first with 4096 neurons, the second also with 4096\\nneurons, and the final layer corresponding to the number of classes. Dropout is applied before the first and second\\nfully connected layers to prevent overfitting. The model is adapted for single-channel grayscale images and the smaller\\nspatial dimensions of the MNIST and FashionMNIST datasets. The use of ReLU activation functions, dropout, and\\noverlapping pooling makes AlexNet robust against overfitting and capable of learning complex patterns in image data.\\nIn the context of MNIST and FashionMNIST, AlexNet’s architecture, although more complex than necessary for such\\nsimple datasets, provides an excellent baseline for performance due to its depth and capacity to learn intricate features.\\nFigure 4: The modified AlexNet architecture used in the implementation of the experiments. Three convolution layers,\\nfirst two followed by ReLU activation and max pool layers were used. Finally, three fully connected layers were used to\\nclassify the images and get output probabilities from the model.\\n4.3.3 Ensemble of AlexNet and LeNet\\nThe ensemble method combines the strengths of both LeNet and AlexNet to achieve better generalization. By averaging\\nthe output probabilities of the final layer from both models, the ensemble captures a more robust representation of the\\ndata. The ensembling can be mathematically represented by Equation 16 as:\\npensemble (y|x) =1\\n2(pLeNet (y|x) +pAlexNet (y|x)), (16)\\nwhere pensemble (y|x)is the probability of the label ygiven input xfor the ensemble, and pAlexNet (y|x)andpLeNet (y|x)\\nare the probabilities given by the individual AlexNet and LeNet models, respectively.\\nThis averaging process smooths out the predictions, making the ensemble less likely to overfit to noise in the dataset\\nand often resulting in improved classification accuracy on test data. The use of these models and their ensemble as\\nteacher models in our experiments aims to provide comprehensive learning signals for the student QNN, leveraging the\\nclassical CNNs’ ability to extract hierarchical features from image data.\\n4.4 Parameterized Quantum Circuits\\nIn our study, we employed two Parameterized Quantum Circuits (PQCs): one with 3 qubits and another with 4 qubits.\\nPQCs are central to quantum machine learning and variational algorithms, where the parameters (often denoted as θ) of\\nquantum gates are tuned to minimize a cost function. The PQCs that are constructed and used in our experiments are\\nshown in Figure 5. The 3-qubit PQC is constructed using Hadamard gates (H gates) and rotation gates around the\\ny-axis (Ry gates), which are parameterized by angles θ. These angles are randomly initialized and optimized during\\nthe learning process. The circuit provides a compact yet expressive model that can capture the complexity needed for\\nsmaller-scale quantum tasks. Similarly, the 4-qubit PQC consists of a sequence of Ry gates with randomly initialized\\nparameters θ. With an additional qubit, the 4-qubit circuit can represent a larger state space, allowing it to capture more\\ncomplex patterns and correlations in the data.\\nThe use of these two circuits is vital with respect to the flexibility and scalability of the methodology. A 3-qubit model\\ndemonstrates how our method performs with a minimal number of qubits, which is important for near-term quantum\\ndevices with limited qubit resources. The 4-qubit model, on the other hand, tests the capability of our method to\\n11RESEARCH ARTICLE\\n(a)\\n (b)\\nFigure 5: Parameterized Quantum Circuits used in the experiments. (a) A simple 3-qubit PQC with H and Ry gates\\nparameterized by θ. (b) A simple 4-qubit PQC with H and Ry gates parameterized by θ.\\nscale to a slightly larger system, which is crucial for assessing performance in more demanding tasks. Together, they\\ndemonstrate generalizability and broader application potential across different quantum system sizes, which is essential\\nfor practical quantum machine learning implementations.\\n4.5 Performance Metrics\\nTo evaluate the performance of our quantum and classical models, we rely on accuracy as the primary metric. Accuracy\\nis a suitable measure in the context of balanced datasets, such as the ones we have crafted through our filtration process.\\nThe accuracy of a model is calculated as the ratio of correctly predicted instances to the total number of instances, as\\nshown in Equation 17:\\nAccuracy =Number of correct predictions\\nTotal number of predictions. (17)\\nSince our filtered datasets are balanced with an equal number of samples from each class, accuracy serves as a reliable\\nmetric. In datasets where classes are imbalanced, metrics such as precision, recall, or the F1 score might be more\\nappropriate. However, for balanced datasets, accuracy straightforwardly reflects the model’s capability to classify new\\ndata points correctly. Accuracy is not only a clear and interpretable metric but also practical from a computational\\nstandpoint. It allows us to clearly demonstrate the effectiveness of our models without incurring additional computational\\ncosts or complexity. Accuracy provides a direct measure of a model’s classification performance and is a widely used\\nmetric in machine learning research for balanced datasets, making it an appropriate choice for this study.\\n4.6 Implementation Details\\nIn our experimental setup, we employed PyTorch [ 63] as the framework for implementing neural network models, with\\nthe Qiskit QASM Simulator [ 64] used for conducting quantum simulations. Hyperparameter optimization was used,\\nenabling the selection of optimal values for the optimizer, learning rate, loss function, temperature for KL-Divergence,\\nand lambda ( λ) ( Equation 6 ). The Adam optimizer [ 56] was chosen for its effectiveness in handling noisy problems\\nand sparse gradients. We set the learning rate to 0.001 and limited the training to 10 epochs to prevent overfitting\\nwhile allowing sufficient model training. The loss function employed was Cross-Entropy Loss [ 51] for classification,\\ncomplemented by KL-Divergence for the knowledge distillation process, with a temperature setting of 3 to soften the\\noutput distributions. A λvalue of 0.9 was utilized to weigh the distillation loss in the overall loss function appropriately.\\nFor the quantum experiments, we performed 5000 shots for each measurement to approximate the expectation values\\nwith high accuracy and interpret them as probability distributions. These hyperparameters were meticulously chosen\\nbased on extensive hyperparameter optimization to ensure the best performing models.\\n4.7 Results\\nThe results of the conducted experiments are presented in the two comprehensive tables Table 1 and Table 2, which\\ndetail the performance of various architectures across the filtered MNIST and FashionMNIST datasets. The results are\\ncategorized into three distinct groups: Student, Teacher, and Distillation, to demonstrate the effectiveness of knowledge\\n12RESEARCH ARTICLE\\nTable 1: Performance of Quantum and Classical Architectures on MNIST dataset variants. The table is divided into\\nStudent, Teacher, and Distillation categories to exhibit the direct training, teacher, and knowledge distillation results\\nrespectively.\\nCategory Architecture DatasetAccuracy\\n(%)+∆for\\nStudent\\nStudentQuantum, 3 qubit MNIST 0-5 96.56 -\\nQuantum, 3 qubit MNIST 0-7 93.75 -\\nQuantum, 4 qubit MNIST 0-7 93.83 -\\nTeacherClassical, AlexNet MNIST 0-5 99.72 -\\nClassical, AlexNet MNIST 0-7 99.51 -\\nClassical, LeNet MNIST 0-5 99.50 -\\nClassical, LeNet MNIST 0-7 99.16 -\\nClassical, Ensemble MNIST 0-5 99.73 -\\nClassical, Ensemble MNIST 0-7 99.58 -\\nDistillationTeacher: AlexNet, Student: Quantum, 3 qubit MNIST 0-5 97.44 0.88%\\nTeacher: AlexNet, Student: Quantum, 3 qubit MNIST 0-7 94.92 1.17%\\nTeacher: AlexNet, Student: Quantum, 4 qubit MNIST 0-7 94.17 0.34%\\nTeacher: LeNet, Student: Quantum, 3 qubit MNIST 0-5 97.89 1.33%\\nTeacher: LeNet, Student: Quantum, 3 qubit MNIST 0-7 93.75 0.00%\\nTeacher: LeNet, Student: Quantum, 4 qubit MNIST 0-7 94.33 0.50%\\nTeacher: Ensemble, Student: Quantum, 3 qubit MNIST 0-5 97.33 0.77%\\nTeacher: Ensemble, Student: Quantum, 3 qubit MNIST 0-7 95.42 1.67%\\nTeacher: Ensemble, Student: Quantum, 4 qubit MNIST 0-7 94.00 0.17%\\nTable 2: Performance of Quantum and Classical Architectures on FashionMNIST dataset variants. Similar to MNIST,\\nthe results are divided into the three categories to provide insights into the quantum models’ capabilities with and\\nwithout the influence of knowledge distillation.\\nCategory Architecture DatasetAccuracy\\n(%)+∆for\\nStudent\\nStudentQuantum, 3 qubit FashionMNIST 0-5 65.00 -\\nQuantum, 3 qubit FashionMNIST 0-7 68.00 -\\nQuantum, 4 qubit FashionMNIST 0-7 74.25 -\\nTeacherClassical, AlexNet FashionMNIST 0-5 94.90 -\\nClassical, AlexNet FashionMNIST 0-7 91.64 -\\nClassical, LeNet FashionMNIST 0-5 91.57 -\\nClassical, LeNet FashionMNIST 0-7 85.97 -\\nClassical, Ensemble FashionMNIST 0-5 95.67 -\\nClassical, Ensemble FashionMNIST 0-7 91.79 -\\nDistillationTeacher: AlexNet, Student: Quantum, 3 qubit FashionMNIST 0-5 83.44 18.44%\\nTeacher: AlexNet, Student: Quantum, 3 qubit FashionMNIST 0-7 73.42 5.42%\\nTeacher: AlexNet, Student: Quantum, 4 qubit FashionMNIST 0-7 75.42 1.17%\\nTeacher: LeNet, Student: Quantum, 3 qubit FashionMNIST 0-5 77.00 12.00%\\nTeacher: LeNet, Student: Quantum, 3 qubit FashionMNIST 0-7 68.67 0.67%\\nTeacher: LeNet, Student: Quantum, 4 qubit FashionMNIST 0-7 75.83 1.58%\\nTeacher: Ensemble, Student: Quantum, 3 qubit FashionMNIST 0-5 67.89 2.89%\\nTeacher: Ensemble, Student: Quantum, 3 qubit FashionMNIST 0-7 74.42 6.42%\\nTeacher: Ensemble, Student: Quantum, 4 qubit FashionMNIST 0-7 72.50 0.00%\\ntransfer in quantum neural network training. The ‘Student’ category represents the quantum models directly trained on\\nthe datasets. The ‘Teacher’ category includes the performance of classical convolutional neural networks, serving as the\\nteacher models to guide the students during distillation. Lastly, the ‘Distillation’ category shows the outcomes of the\\nstudent quantum models when knowledge is distilled from the classical teacher models, highlighting the enhancement\\nin performance due to the distillation process. The “+ ∆for Student\" represents the positive change in accuracy of\\n13RESEARCH ARTICLE\\nthe student quantum models upon training by knowledge distillation. The Architecture column shows the kind of\\nmodel used, and in case of knowledge distillation, the teacher and student models used in each experiment. In case\\nof the Student and Teacher categories, the positive increase for student is not relevant, hence kept blank using ‘-’. As\\ndepicted in Figure 6, the loss curves for the distillation training of the two student models for 3 and 4 qubits for the\\nFashionMNIST subsets are presented. Each subfigure illustrates the decreasing trend of training and validation losses\\nover epochs, indicating effective learning and generalization of the models. The consistent decline in loss values across\\nall models signifies the models’ ability to learn and adapt from the distilled knowledge, underlining the potential of\\nquantum neural networks in assimilating complex patterns from classical pre-trained networks. The convergence of\\nthese curves demonstrates the learning of the models, indicative of the successful transfer of knowledge from the teacher\\nmodels to their quantum counterparts.\\n(a)\\n (b)\\nFigure 6: Loss curves for distillation training of the two best student models for the FashionMNIST dataset, showcasing\\nthe efficiency of knowledge transfer. (a) Represents the learning curve of the best performing 3-qubit model with\\nFashionMNIST 0-5 and AlexNet as the teacher model. (b) Represents the best performing 4-qubit model with\\nFashionMNIST 0-7 and LeNet as the teacher model.\\n5 Findings and Discussion\\nBased on the experimental results of the previous section, in this section, we present our findings and discuss upon\\nthem. Based on Table 1, Table 2 and Figure 7, we can observe the followings: (1)When used as a teacher model\\nin knowledge distillation, the LeNet and ensemble models exhibit superior performance compared to the simplified\\nAlexNet architecture. This is likely because LeNet was explicitly designed for grayscale images like those in the\\nMNIST and FashionMNIST datasets, allowing it to capture features more effectively than the more complex AlexNet,\\nwhich required simplification for our experiments. (2)We observe that the average improvement in performance on the\\nFashionMNIST dataset is greater than that on the MNIST dataset. This could be due to FashionMNIST’s inherently\\nlower baseline accuracies, which leaves more room for improvement. This disparity also highlights the third observation:\\n(3)FashionMNIST, with its more varied and complex images, poses a greater classification challenge than MNIST. Yet,\\nour knowledge distillation (KD) approach manages to improve the performance of quantum models on FashionMNIST\\nsignificantly, indicating the robustness and applicability of KD in enhancing models trained on complex datasets. (4)It\\ncan also be observed that the 4-qubit student models do not significantly outperform the 3-qubit models. This suggests\\nthat increasing the complexity of the quantum model slightly does not necessarily lead to better learning. This is a\\ntestament to our main research objective: reducing the need for quantum resources while improving and maintaining\\nrobust performance. (5)The ensemble model performs best for the MNIST 0-7 and FashionMNIST 0-7 datasets, which\\nare the most extensive datasets in our experiments. This model’s superior performance is attributed to its ability to\\nintegrate diverse features and learning aspects from multiple classical models, thereby enhancing its predictive accuracy.\\nInterestingly, this advantage is more highlighted in larger datasets. In contrast, the performance gains in smaller datasets\\nare less remarkable, suggesting that ensemble models excel when there is a wealth of variation to learn from, while in\\nmore limited datasets, the opportunity to leverage their full potential is not as readily available. (6)Lastly, we note that\\nthe performance improvements through distillation are consistent across different dataset filtrations. This consistency\\nunderscores the potential for KD to be used in various quantum machine learning applications, especially as quantum\\nhardware continues to evolve and allows for training on larger and more diverse datasets.\\n14RESEARCH ARTICLE\\n(a)\\n (b)\\nFigure 7: Bar plot visually showing the improvement of the student quantum models compared to each distillation\\nexperiment performed on the (a) MNIST dataset subsets and (b) FashionMNIST dataset subsets. Each plot is divided\\ninto three groups for the three student quantum models. The grey colored bar in each group represents the baseline\\nperformance of the student models for the subset mentioned. Subsequent bars represent the improvement for each of\\nthe teacher models AlexNet, LeNet and Ensemble model.\\nThe findings signify the potential for classical machine learning architectures to substantially enhance the performance\\nof quantum models through knowledge distillation. The compatibility of the teacher model with the data is crucial,\\nas evidenced by the superior performance of LeNet and ensemble models tailored for grayscale image datasets. The\\nadvancements in knowledge distillation demonstrated in this study are particularly promising for complex datasets,\\nsuggesting that as quantum computational resources grow, the scope of quantum machine learning applications will\\nbroaden significantly. The use of established classical models like ResNet [ 65], VGG [ 66], and InceptionNet [ 67] as\\nteachers in quantum environments might open up new possibilities for cross-domain applications, reducing the reliance\\non the development of large quantum models and leveraging the extensive resources of classical pre-trained models.\\nThis research paves the way for future endeavors where quantum models are integrated into everyday tasks, marking a\\nsignificant step forward in the evolution of quantum computing applications.\\n6 Conclusion and Future Work\\nIn this paper, we presented a new method for transferring knowledge from classical neural networks to quantum\\nneural networks via knowledge distillation, showcasing a promising avenue for leveraging classical machine learning\\narchitectures within quantum computing paradigms. By tweaking and leveraging classical models like LeNet, AlexNet\\nand their ensemble to act as teachers, we demonstrated significant improvements in quantum model performances on\\nboth MNIST and FashionMNIST datasets, with higher enhancements observed in the more complex FashionMNIST\\ndataset. The results showed an average improvement of 0.80% on the MNIST dataset and 5.40% on the FashionMNIST\\ndataset.\\nOur results indicate the potential for classical-to-quantum knowledge distillation in a variety of applications. In natural\\nlanguage processing tasks, where data representation and model interpretability are paramount, the method could\\nbe utilized in developing more efficient quantum models. The healthcare sector could also benefit immensely, with\\napplications ranging from disease prediction to clinical decision-making, where quantum-enhanced models could lead\\nto faster and more accurate diagnoses. Furthermore, the principles of this research hold great promise for advancing\\nobject identification, tracking, and detection tasks, crucial in areas such as autonomous driving and surveillance. As\\nquantum computing continues to evolve, the techniques outlined in this study are of paramount importance in bridging\\nthe gap between classical machine learning successes and quantum computing’s potential, fostering advancements\\nacross an array of domains and setting a precedent for future quantum machine learning research.\\n15RESEARCH ARTICLE\\nAcknowledgements\\nWe would like to thank Mr. Md Shawmoon Azad, research assistant of Dr. Mahdy Rahman Chowdhury, for his valuable\\nfeedback, suggestions, and assistance in reviewing, cross-checking, and validating results in this research. His expertise\\nand insights have greatly improved the quality and clarity of this paper.\\nReferences\\n[1]Rafał Potempa and Sebastian Porebski. Comparing concepts of quantum and classical neural network models for\\nimage classification task. In Progress in Image Processing, Pattern Recognition and Communication Systems:\\nProceedings of the Conference (CORES, IP&C, ACS)-June 28-30 2021 12 , pages 61–71. Springer, 2022.\\n[2]Mahabubul Alam and Swaroop Ghosh. Qnet: A scalable and noise-resilient quantum neural network architecture\\nfor noisy intermediate-scale quantum computers. Frontiers in Physics , 9:702, 2022.\\n[3]Patrick J Coles. Seeking quantum advantage for neural networks. Nature Computational Science , 1(6):389–390,\\n2021.\\n[4]Kerstin Beer, Dmytro Bondarenko, Terry Farrelly, Tobias J Osborne, Robert Salzmann, Daniel Scheiermann, and\\nRamona Wolf. Training deep quantum neural networks. Nature communications , 11(1):808, 2020.\\n[5]Yuxuan Du, Min-Hsiu Hsieh, Tongliang Liu, Shan You, and Dacheng Tao. Learnability of quantum neural\\nnetworks. PRX Quantum , 2(4):040337, 2021.\\n[6]M Cerezo, Guillaume Verdon, Hsin-Yuan Huang, Lukasz Cincio, and Patrick J Coles. Challenges and opportunities\\nin quantum machine learning. Nature Computational Science , 2(9):567–576, 2022.\\n[7]Yunseok Kwak, Won Joon Yun, Soyi Jung, and Joongheon Kim. Quantum neural networks: Concepts, applications,\\nand challenges. In 2021 Twelfth International Conference on Ubiquitous and Future Networks (ICUFN) , pages\\n413–416. IEEE, 2021.\\n[8]Renxin Zhao and Shi Wang. A review of quantum neural networks: methods, models, dilemma. arXiv preprint\\narXiv:2109.01840 , 2021.\\n[9]Mahabubul Alam, Satwik Kundu, and Swaroop Ghosh. Knowledge distillation in quantum neural network using\\napproximate synthesis. In Proceedings of the 28th Asia and South Pacific Design Automation Conference , pages\\n639–644, 2023.\\n[10] Sungho Shin, Yoonho Boo, and Wonyong Sung. Knowledge distillation for optimization of quantized deep neural\\nnetworks. In 2020 IEEE Workshop on Signal Processing Systems (SiPS) , pages 1–6. IEEE, 2020.\\n[11] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint\\narXiv:1503.02531 , 2015.\\n[12] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. Interna-\\ntional Journal of Computer Vision , 129:1789–1819, 2021.\\n[13] Li Deng. The mnist database of handwritten digit images for machine learning research [best of the web]. IEEE\\nsignal processing magazine , 29(6):141–142, 2012.\\n[14] Han Xiao, Kashif Rasul, and Roland V ollgraf. Fashion-mnist: a novel image dataset for benchmarking machine\\nlearning algorithms. arXiv preprint arXiv:1708.07747 , 2017.\\n[15] Frank Arute, Kunal Arya, Ryan Babbush, Dave Bacon, Joseph C Bardin, Rami Barends, Rupak Biswas, Sergio\\nBoixo, Fernando GSL Brandao, David A Buell, et al. Quantum supremacy using a programmable superconducting\\nprocessor. Nature , 574(7779):505–510, 2019.\\n[16] Lars S Madsen, Fabian Laudenbach, Mohsen Falamarzi Askarani, Fabien Rortais, Trevor Vincent, Jacob FF\\nBulmer, Filippo M Miatto, Leonhard Neuhaus, Lukas G Helt, Matthew J Collins, et al. Quantum computational\\nadvantage with a programmable photonic processor. Nature , 606(7912):75–81, 2022.\\n[17] Peter W Shor. Scheme for reducing decoherence in quantum computer memory. Physical review A , 52(4):R2493,\\n1995.\\n[18] Kishor Bharti, Alba Cervera-Lierta, Thi Ha Kyaw, Tobias Haug, Sumner Alperin-Lea, Abhinav Anand, Matthias\\nDegroote, Hermanni Heimonen, Jakob S Kottmann, Tim Menke, et al. Noisy intermediate-scale quantum\\nalgorithms. Reviews of Modern Physics , 94(1):015004, 2022.\\n[19] AMJ Zwerver, T Krähenmann, TF Watson, Lester Lampert, Hubert C George, Ravi Pillarisetty, SA Bojarski,\\nPayam Amin, SV Amitonov, JM Boter, et al. Qubits made by advanced semiconductor manufacturing. Nature\\nElectronics , 5(3):184–190, 2022.\\n16RESEARCH ARTICLE\\n[20] Hajime Okamoto, Adrien Gourgout, Chia-Yuan Chang, Koji Onomitsu, Imran Mahboob, Edward Yi Chang,\\nand Hiroshi Yamaguchi. Coherent phonon manipulation in coupled mechanical resonators. Nature Physics ,\\n9(8):480–484, 2013.\\n[21] Mohammad H Amin, Evgeny Andriyash, Jason Rolfe, Bohdan Kulchytskyy, and Roger Melko. Quantum\\nboltzmann machine. Physical Review X , 8(2):021050, 2018.\\n[22] Jacob Biamonte, Peter Wittek, Nicola Pancotti, Patrick Rebentrost, Nathan Wiebe, and Seth Lloyd. Quantum\\nmachine learning. Nature , 549(7671):195–202, 2017.\\n[23] Amira Abbas, David Sutter, Christa Zoufal, Aurélien Lucchi, Alessio Figalli, and Stefan Woerner. The power of\\nquantum neural networks. Nature Computational Science , 1(6):403–409, 2021.\\n[24] V ojtˇech Havlí ˇcek, Antonio D Córcoles, Kristan Temme, Aram W Harrow, Abhinav Kandala, Jerry M Chow, and\\nJay M Gambetta. Supervised learning with quantum-enhanced feature spaces. Nature , 567(7747):209–212, 2019.\\n[25] Valeria Saggio, Beate E Asenbeck, Arne Hamann, Teodor Strömberg, Peter Schiansky, Vedran Dunjko, Nicolai\\nFriis, Nicholas C Harris, Michael Hochberg, Dirk Englund, et al. Experimental quantum speed-up in reinforcement\\nlearning agents. Nature , 591(7849):229–233, 2021.\\n[26] Marco Cerezo, Andrew Arrasmith, Ryan Babbush, Simon C Benjamin, Suguru Endo, Keisuke Fujii, Jarrod R\\nMcClean, Kosuke Mitarai, Xiao Yuan, Lukasz Cincio, et al. Variational quantum algorithms. Nature Reviews\\nPhysics , 3(9):625–644, 2021.\\n[27] Adam Bouland, Wim van Dam, Hamed Joorati, Iordanis Kerenidis, and Anupam Prakash. Prospects and challenges\\nof quantum finance. arXiv preprint arXiv:2011.06492 , 2020.\\n[28] Ryuji Takagi, Suguru Endo, Shintaro Minagawa, and Mile Gu. Fundamental limits of quantum error mitigation.\\nnpj Quantum Information , 8(1):114, 2022.\\n[29] Dayue Qin, Yanzhu Chen, and Ying Li. Error statistics and scalability of quantum error mitigation formulas. npj\\nQuantum Information , 9(1):35, 2023.\\n[30] Phalgun Lolur, Mårten Skogh, Werner Dobrautz, Christopher Warren, Janka Biznárová, Amr Osman, Giovanna\\nTancredi, Goran Wendin, Jonas Bylander, and Martin Rahm. Reference-state error mitigation: A strategy for\\nhigh accuracy quantum computation of chemistry. Journal of Chemical Theory and Computation , 19(3):783–789,\\n2023.\\n[31] Abhinav Kandala, Kristan Temme, Antonio D Córcoles, Antonio Mezzacapo, Jerry M Chow, and Jay M Gambetta.\\nError mitigation extends the computational reach of a noisy quantum processor. Nature , 567(7749):491–495,\\n2019.\\n[32] Armands Strikis, Dayue Qin, Yanzhu Chen, Simon C Benjamin, and Ying Li. Learning-based quantum error\\nmitigation. PRX Quantum , 2(4):040330, 2021.\\n[33] Daniel Bultrini, Max Hunter Gordon, Piotr Czarnik, Andrew Arrasmith, M Cerezo, Patrick J Coles, and Lukasz\\nCincio. Unifying and benchmarking state-of-the-art quantum error mitigation techniques. Quantum , 7:1034, 2023.\\n[34] Angus Lowe, Max Hunter Gordon, Piotr Czarnik, Andrew Arrasmith, Patrick J Coles, and Lukasz Cincio. Unified\\napproach to data-driven quantum error mitigation. Physical Review Research , 3(3):033098, 2021.\\n[35] Anita Weidinger, Glen Bigan Mbeng, and Wolfgang Lechner. Error mitigation for quantum approximate optimiza-\\ntion. arXiv preprint arXiv:2301.05042 , 2023.\\n[36] Marco Fellous-Asiani, Jing Hao Chai, Yvain Thonnart, Hui Khoon Ng, Robert S Whitney, and Alexia Auffèves.\\nOptimizing resource efficiencies for scalable full-stack quantum computers. PRX Quantum , 4(4):040319, 2023.\\n[37] Margaret Martonosi and Martin Roetteler. Next steps in quantum computing: Computer science’s role. arXiv\\npreprint arXiv:1903.10541 , 2019.\\n[38] Olawale Ayoade, Pablo Rivas, and Javier Orduz. Artificial intelligence computing at the quantum level. Data ,\\n7(3):28, 2022.\\n[39] Annarita Giani and Zachary Eldredge. Quantum computing opportunities in renewable energy. SN Computer\\nScience , 2(5):393, 2021.\\n[40] Remmy Zen, Long My, Ryan Tan, Frédéric Hébert, Mario Gattobigio, Christian Miniatura, Dario Poletti, and\\nStéphane Bressan. Transfer learning for scalability of neural-network quantum states. Physical Review E ,\\n101(5):053301, 2020.\\n[41] Justin S Smith, Benjamin T Nebgen, Roman Zubatyuk, Nicholas Lubbers, Christian Devereux, Kipton Barros,\\nSergei Tretiak, Olexandr Isayev, and Adrian Roitberg. Outsmarting quantum chemistry through transfer learning.\\n2018.\\n17RESEARCH ARTICLE\\n[42] Justin S Smith, Benjamin T Nebgen, Roman Zubatyuk, Nicholas Lubbers, Christian Devereux, Kipton Barros,\\nSergei Tretiak, Olexandr Isayev, and Adrian E Roitberg. Approaching coupled cluster accuracy with a general-\\npurpose neural network potential through transfer learning. Nature communications , 10(1):2903, 2019.\\n[43] Andrea Mari, Thomas R Bromley, Josh Izaac, Maria Schuld, and Nathan Killoran. Transfer learning in hybrid\\nclassical-quantum neural networks. Quantum , 4:340, 2020.\\n[44] Muhammad Junaid Umer, Javeria Amin, Muhammad Sharif, Muhammad Almas Anjum, Faisal Azam, and\\nJamal Hussain Shah. An integrated framework for covid-19 classification based on classical and quantum transfer\\nlearning from a chest radiograph. Concurrency and Computation: Practice and Experience , 34(20):e6434, 2022.\\n[45] Mine Kaya and Shima Hajimirza. Using a novel transfer learning method for designing thin film solar cells with\\nenhanced quantum efficiencies. Scientific reports , 9(1):5034, 2019.\\n[46] Javeria Amin, Muhammad Almas Anjum, Muhammad Sharif, Saima Jabeen, Seifedine Kadry, Pablo Moreno Ger,\\net al. A new model for brain tumor detection using ensemble transfer learning and quantum variational classifier.\\nComputational intelligence and neuroscience , 2022, 2022.\\n[47] T Tamilvizhi, R Surendran, K Anbazhagan, and K Rajkumar. Quantum behaved particle swarm optimization-based\\ndeep transfer learning model for sugarcane leaf disease detection and classification. Mathematical Problems in\\nEngineering , 2022, 2022.\\n[48] Dipendra Jha, Kamal Choudhary, Francesca Tavazza, Wei-keng Liao, Alok Choudhary, Carelyn Campbell, and\\nAnkit Agrawal. Enhancing materials property prediction by leveraging computational and experimental data using\\ndeep transfer learning. Nature communications , 10(1):5316, 2019.\\n[49] Jun Qi and Javier Tejedor. Classical-to-quantum transfer learning for spoken command recognition based on\\nquantum neural networks. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\\nProcessing (ICASSP) , pages 8627–8631. IEEE, 2022.\\n[50] Juhyeon Kim, Joonsuk Huh, and Daniel K Park. Classical-to-quantum convolutional neural network transfer\\nlearning. Neurocomputing , 555:126643, 2023.\\n[51] Zhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy\\nlabels. Advances in neural information processing systems , 31, 2018.\\n[52] Solomon Kullback and Richard A Leibler. On information and sufficiency. The annals of mathematical statistics ,\\n22(1):79–86, 1951.\\n[53] John V on Neumann. Mathematical foundations of quantum mechanics: New edition , volume 53. Princeton\\nuniversity press, 2018.\\n[54] Jin-Guo Liu and Lei Wang. Differentiable learning of quantum circuit born machines. Physical Review A ,\\n98(6):062324, 2018.\\n[55] Rachel Ward, Xiaoxia Wu, and Leon Bottou. Adagrad stepsizes: Sharp convergence over nonconvex landscapes.\\nThe Journal of Machine Learning Research , 21(1):9047–9076, 2020.\\n[56] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 ,\\n2014.\\n[57] Mahesh Chandra Mukkamala and Matthias Hein. Variants of rmsprop and adagrad with logarithmic regret bounds.\\nInInternational conference on machine learning , pages 2545–2553. PMLR, 2017.\\n[58] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning and stochastic\\noptimization. Journal of machine learning research , 12(7), 2011.\\n[59] Maria Schuld and Nathan Killoran. Quantum machine learning in feature hilbert spaces. Physical review letters ,\\n122(4):040504, 2019.\\n[60] Edward Farhi and Hartmut Neven. Classification with quantum neural networks on near term processors. arXiv\\npreprint arXiv:1802.06002 , 2018.\\n[61] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document\\nrecognition. Proceedings of the IEEE , 86(11):2278–2324, 1998.\\n[62] Alex Krizhevsky. One weird trick for parallelizing convolutional neural networks. arXiv preprint arXiv:1404.5997 ,\\n2014.\\n[63] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen,\\nZeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep\\nlearning library. Advances in neural information processing systems , 32, 2019.\\n18RESEARCH ARTICLE\\n[64] Gadi Aleksandrowicz, Thomas Alexander, Panagiotis Barkoutsos, Luciano Bello, Yael Ben-Haim, David Bucher,\\nF Jose Cabrera-Hernández, Jorge Carballo-Franquis, Adrian Chen, Chun-Fu Chen, et al. Qiskit: An open-source\\nframework for quantum computing. Accessed on: Mar , 16, 2019.\\n[65] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In\\nProceedings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.\\n[66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition.\\narXiv preprint arXiv:1409.1556 , 2014.\\n[67] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception\\narchitecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern\\nrecognition , pages 2818–2826, 2016.\\n19'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main research questions of this study?\"\n",
    "docs = document_search.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
